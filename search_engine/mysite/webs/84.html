Xiaolin HU 
     Assistant Professor 
     Department of Computer Science and Technology 
     Joined Department: 2009 
     Email: xlhu@tsinghua.edu.cn 
     URL: 
     Phone: +86-10-62795869 
     Fax: +86-10-62782266 
 
 Education background 
 Bachelor of Automotive Engineering, Wuhan University of Technology, Wuhan, China, 2001; 
 Master of Automotive Engineering, Wuhan University of Technology, Wuhan, China, 2004; 
 Ph.D. in Automation and Computer-Aided Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China, 2007. 
  Social service 
 ISNN 2009: Publication Chair (2009); 
 ISNN 2010: Publicity Chair (2010); 
 IWACI 2010: Publicity Chair (2010); 
 ICICIP 2010: Publicity Chair (2010). 
 Areas of Research Interests/ Research Projects 
 Artificial Neural Networks 
 Computational Neuroscience 
 National Natural Science Foundation of China: Design of Recurrent Neural Network Groups for Optimization based on KKT Conditions (2009-2011); 
 China Postdoctoral Science Foundation Grant: Theory and Applications of a Class of Recurrent Neural Networks for Optimization (2008-2009); 
 China Postdoctoral Science Foundation Grant: A Class of Recurrent Neural Networks for Optimization Differing in Connections and Related Gaussian Attractor Networks (2008-2009); 
 TNList Basic Research Grant: Neural and Cognitive Computation (2010-2011). 
 Research Status 
 Since my Ph.D. studies, I have been doing researches in artificial neural networks. My main interest at that time was recurrent neural networks for solving optimization problems and its related problems. I focused on two issues: designing new recurrent neural networks and mining some new properties from existing networks. Essentially, the former requires creativity, while the latter requires profound mathematical skills. In recent years, by comparing some state-of-the-art models in this area, we proposed a unified framework for designing such models. The main idea is to change connections between some blocks of existing models while keeping the blocks themselves intact. A set of new models can be obtained simultaneously in this way, and some of them may outperform the original one. The major contribution of this framework is two-fold. On the one hand, it unifies a few well-known models proposed independently. On the other hand, it greatly speeds up the design process of such models. 
 In 2007, I came to Tsinghua University as a postdoctoral researcher, working for Prof. Bo Zhang. Following his suggestions, I began to pay attention to cognitive neuroscience, and shifted my interest to computational neuroscience, a branch of neuroscience closely related to information science. Recently, we designed a novel neural network model for recognition and memory. Combing with the famous HMAX model, this model can mimic many important physiological and psychological experimental results in neuroscience. This research sets an example for our future interdisciplinary researches between information science and neuroscience. 
 In December 2009, Tsinghua University approved our application for establishing a Chair Professor Group, led by Dr. Michael Merzenich from UCSF, a member of the US National Academy of Sciences, to help us in neuroscience-related researches. Seven famous neuroscientists have joined the group. We are optimistic for making great progresses in the forthcoming years. 
 Academic Achievement 
 [1] X. Hu, C. Sun, and B. Zhang. Design of recurrent neural networks for solving constrained least absolute deviation problems. IEEE Transactions on Neural Networks (Regular Papers) (accepted). 
 [2] X. Hu and B. Zhang. A Gaussian attractor network for memory and recognition with experience-dependent Learning. Neural Computation, vol. 22, no. 5, pp. 1333-1357, May 2010. 
 [3] X. Hu and B. Zhang. An alternative recurrent neural network for solving variational inequalities and related optimization problems. IEEE Transactions on Systems, Man and Cybernetics - Part B, vol. 39, no. 6, pp. 1640-1645, Dec. 2009. 
 [4] X. Hu and B. Zhang. A new recurrent neural network for solving convex quadratic programming problems with an application to the k-winners-take-all problem. IEEE Transactions on Neural Networks (Regular Papers), vol. 20, no. 4, pp. 654-664, April 2009. 
 [5] X. Hu and J. Wang. An improved dual neural network for solving a class of quadratic programming problems and its k-winners-take-all application. IEEE Transactions on Neural Networks (Regular Papers), vol. 19, no. 12, pp. 2022-2031, Dec. 2008. 
 [6] X. Hu and J. Wang.  Design of general projection neural networks for solving monotone linear variational inequalities and linear and quadratic optimization problems . IEEE Transactions on Systems, Man and Cybernetics - Part B, vol. 37, no. 5, pp. 1414-1421, 2007. 
 [7] X. Hu and J. Wang.  Solving generally constrained generalized linear variational inequalities using the  general projection neural networks. IEEE Transactions on Neural Networks (Regular Papers), vol. 18, no. 6, pp. 1697-1708, 2007. 
 [8] X. Hu and J. Wang. A recurrent neural network for solving a class of general variational inequalities. IEEE Transactions on Systems, Man and Cybernetics - Part B (Regular Papers), vol. 37, no. 3, pp. 528-539, 2007. 
 [9] X. Hu and J. Wang. Solving pseudomonotone variational inequalities and pseudoconvex optimization problems using the projection neural network. IEEE Transactions on Neural Networks (Regular Papers), vol. 17, no. 6, pp. 1487-1499, 2006. 
 
                   
                   
             
                                       
        
     
   
   
   

 
  Copyright  2009-2011 Department of Computer Science and Technology  All Rights Reserved 
    
  
  
 
 
 